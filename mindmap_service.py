from typing import List, Dict, Tuple
from models import MindmapData, MindmapNode, MindmapLink, Page
from database import optimized_db_manager as db_manager
from llm_service import calculate_keyword_similarity, get_common_keywords
from config import config
import logging

logger = logging.getLogger(__name__)

class MindmapService:
    def __init__(self):
        self.threshold = config.MINDMAP_THRESHOLD
        self.max_depth = config.MINDMAP_MAX_DEPTH
    
    def generate_mindmap_data(self, parent_page_id: str, threshold: float = None, max_depth: int = None) -> MindmapData:
        """ë§ˆì¸ë“œë§µ ë°ì´í„° ìƒì„±"""
        threshold = threshold or self.threshold
        max_depth = max_depth or self.max_depth
        
        logger.info(f"ë§ˆì¸ë“œë§µ ìƒì„± ì‹œì‘: parent_id={parent_page_id}, threshold={threshold}")
        
        # ë¶€ëª¨ í˜ì´ì§€ ì¡°íšŒ
        parent_page = db_manager.get_page(parent_page_id)
        if not parent_page:
            raise ValueError(f"ë¶€ëª¨ í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {parent_page_id}")
        
        logger.info(f"ë¶€ëª¨ í˜ì´ì§€ ì°¾ìŒ: {parent_page.title} (í‚¤ì›Œë“œ: {len(parent_page.keywords_list)}ê°œ)")
        
        # ëª¨ë“  ê´€ë ¨ í˜ì´ì§€ ì¡°íšŒ
        related_pages = db_manager.get_pages_by_parent(parent_page_id)
        logger.info(f"ê´€ë ¨ í˜ì´ì§€ ì¡°íšŒ ì™„ë£Œ: {len(related_pages)}ê°œ")
        
        # ë¶€ëª¨ í˜ì´ì§€ë„ í¬í•¨
        all_pages = related_pages.copy()
        if parent_page not in all_pages:
            all_pages.append(parent_page)
            
        logger.info(f"ìµœì¢… í˜ì´ì§€ ìˆ˜: {len(all_pages)}ê°œ (ë¶€ëª¨ í˜ì´ì§€ í¬í•¨)")
        
        # ë…¸ë“œ ìƒì„±
        nodes = self._create_nodes(all_pages)
        
        # ë§í¬ ìƒì„±
        links = self._create_links(all_pages, threshold)
        
        # ì¤‘ì‹¬ì„± ê³„ì‚° ë° ë…¸ë“œ í¬ê¸° ì¡°ì •
        self._calculate_centrality(nodes, links)
        
        return MindmapData(
            nodes=nodes,
            links=links,
            center_node=parent_page_id
        )
    
    def _create_nodes(self, pages: List[Page]) -> List[MindmapNode]:
        """í˜ì´ì§€ë¥¼ ë…¸ë“œë¡œ ë³€í™˜"""
        nodes = []
        
        for page in pages:
            node = MindmapNode(
                id=page.page_id,
                title=page.title,
                keywords=page.keywords_list,
                url=page.url or "",
                summary=page.summary or "",
                space_key=getattr(page, 'space_key', None),
                size=10  # ê¸°ë³¸ í¬ê¸°, ë‚˜ì¤‘ì— ì¤‘ì‹¬ì„±ìœ¼ë¡œ ì¡°ì •
            )
            nodes.append(node)
        
        return nodes
    
    def _create_links(self, pages: List[Page], threshold: float) -> List[MindmapLink]:
        """í˜ì´ì§€ ê°„ ë§í¬ ìƒì„±"""
        links = []
        
        for i, page1 in enumerate(pages):
            for j, page2 in enumerate(pages):
                if i >= j:  # ì¤‘ë³µ ë°©ì§€
                    continue
                
                # í‚¤ì›Œë“œ ìœ ì‚¬ë„ ê³„ì‚°
                similarity = calculate_keyword_similarity(
                    page1.keywords_list, 
                    page2.keywords_list
                )
                
                # ì„ê³„ê°’ ì´ìƒì¸ ê²½ìš° ë§í¬ ìƒì„±
                if similarity >= threshold:
                    common_keywords = get_common_keywords(
                        page1.keywords_list, 
                        page2.keywords_list
                    )
                    
                    link = MindmapLink(
                        source=page1.page_id,
                        target=page2.page_id,
                        weight=similarity,
                        common_keywords=common_keywords
                    )
                    links.append(link)
                    
                    # ë°ì´í„°ë² ì´ìŠ¤ì— ê´€ê³„ ì €ì¥
                    try:
                        db_manager.create_relationship(
                            page1.page_id,
                            page2.page_id,
                            similarity,
                            common_keywords
                        )
                    except Exception as e:
                        logger.warning(f"ê´€ê³„ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        
        return links
    
    def _calculate_centrality(self, nodes: List[MindmapNode], links: List[MindmapLink]):
        """ë…¸ë“œ ì¤‘ì‹¬ì„± ê³„ì‚° (ê°„ë‹¨í•œ degree centrality)"""
        if not links:
            return
        
        # ê° ë…¸ë“œì˜ ì—°ê²° ìˆ˜ ê³„ì‚°
        node_degrees = {}
        for node in nodes:
            node_degrees[node.id] = 0
        
        for link in links:
            if link.source in node_degrees:
                node_degrees[link.source] += 1
            if link.target in node_degrees:
                node_degrees[link.target] += 1
        
        # ìµœëŒ€ ì—°ê²° ìˆ˜ ì°¾ê¸°
        max_degree = max(node_degrees.values()) if node_degrees else 1
        
        # ë…¸ë“œ í¬ê¸° ì¡°ì • (10~50 ë²”ìœ„)
        for node in nodes:
            degree = node_degrees.get(node.id, 0)
            normalized_degree = degree / max_degree if max_degree > 0 else 0
            node.size = int(10 + normalized_degree * 40)
    
    def get_page_connections(self, page_id: str) -> List[Dict]:
        """íŠ¹ì • í˜ì´ì§€ì˜ ì—°ê²° ì •ë³´ ì¡°íšŒ"""
        relationships = db_manager.get_relationships(page_id)
        connections = []
        
        for rel in relationships:
            connected_page_id = rel.target_page_id if rel.source_page_id == page_id else rel.source_page_id
            connected_page = db_manager.get_page(connected_page_id)
            
            if connected_page:
                connections.append({
                    'page_id': connected_page.page_id,
                    'title': connected_page.title,
                    'weight': rel.weight,
                    'common_keywords': rel.common_keywords_list
                })
        
        return connections
    
    def update_relationships(self, pages: List[Page]):
        """í˜ì´ì§€ ê´€ê³„ ì—…ë°ì´íŠ¸"""
        logger.info(f"ê´€ê³„ ì—…ë°ì´íŠ¸ ì‹œì‘: {len(pages)} í˜ì´ì§€")
        
        # ê¸°ì¡´ ê´€ê³„ ì‚­ì œ (ì„ íƒì )
        # ì—¬ê¸°ì„œëŠ” ì¤‘ë³µ ìƒì„±ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì²´í¬
        
        for i, page1 in enumerate(pages):
            for j, page2 in enumerate(pages):
                if i >= j:
                    continue
                
                similarity = calculate_keyword_similarity(
                    page1.keywords_list, 
                    page2.keywords_list
                )
                
                if similarity >= self.threshold:
                    common_keywords = get_common_keywords(
                        page1.keywords_list, 
                        page2.keywords_list
                    )
                    
                    try:
                        db_manager.create_relationship(
                            page1.page_id,
                            page2.page_id,
                            similarity,
                            common_keywords
                        )
                    except Exception as e:
                        logger.warning(f"ê´€ê³„ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        
        logger.info("ê´€ê³„ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
    
    def generate_all_pages_mindmap(self, threshold: float = None, limit: int = 100) -> MindmapData:
        """ëª¨ë“  í˜ì´ì§€ì˜ ë§ˆì¸ë“œë§µ ë°ì´í„° ìƒì„±"""
        threshold = threshold or self.threshold
        
        # ëª¨ë“  í˜ì´ì§€ ì¡°íšŒ (ì œí•œ)
        all_pages = db_manager.get_all_pages(limit=limit)
        
        if not all_pages:
            # ë¹ˆ ë§ˆì¸ë“œë§µ ë°˜í™˜
            return MindmapData(
                nodes=[],
                links=[],
                center_node=""
            )
        
        # ê°€ì¥ ë§ì€ í‚¤ì›Œë“œë¥¼ ê°€ì§„ í˜ì´ì§€ë¥¼ ì¤‘ì‹¬ ë…¸ë“œë¡œ ì„¤ì •
        center_page = max(all_pages, key=lambda p: len(p.keywords_list))
        
        # ë…¸ë“œ ìƒì„±
        nodes = self._create_nodes(all_pages)
        
        # ë§í¬ ìƒì„±
        links = self._create_links(all_pages, threshold)
        
        # ì¤‘ì‹¬ì„± ê³„ì‚° ë° ë…¸ë“œ í¬ê¸° ì¡°ì •
        self._calculate_centrality(nodes, links)
        
        logger.info(f"ì „ì²´ ë§ˆì¸ë“œë§µ ìƒì„± ì™„ë£Œ: {len(nodes)}ê°œ ë…¸ë“œ, {len(links)}ê°œ ë§í¬")
        
        return MindmapData(
            nodes=nodes,
            links=links,
            center_node=center_page.page_id
        )
    
    def generate_keyword_mindmap(self, keyword: str, threshold: float = None, limit: int = 100) -> MindmapData:
        """íŠ¹ì • í‚¤ì›Œë“œ ê¸°ë°˜ ë§ˆì¸ë“œë§µ ë°ì´í„° ìƒì„± (í‚¤ì›Œë“œ ë…¸ë“œ ê¸°ë°˜)"""
        threshold = threshold or self.threshold
        
        # í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ í˜ì´ì§€ ì¡°íšŒ
        keyword_pages = db_manager.get_pages_by_keyword(keyword, limit=limit)
        
        if not keyword_pages:
            # ë¹ˆ ë§ˆì¸ë“œë§µ ë°˜í™˜
            return MindmapData(
                nodes=[],
                links=[],
                center_node=""
            )
        
        # ëª¨ë“  í‚¤ì›Œë“œ ìˆ˜ì§‘ ë° ë¹ˆë„ ê³„ì‚°
        keyword_count = {}
        keyword_pages_map = {}  # í‚¤ì›Œë“œë³„ í˜ì´ì§€ ë¦¬ìŠ¤íŠ¸
        
        for page in keyword_pages:
            for kw in page.keywords_list:
                kw_lower = kw.lower()
                keyword_count[kw_lower] = keyword_count.get(kw_lower, 0) + 1
                
                if kw_lower not in keyword_pages_map:
                    keyword_pages_map[kw_lower] = []
                keyword_pages_map[kw_lower].append(page)
        
        # ë¹ˆë„ê°€ ë†’ì€ í‚¤ì›Œë“œë“¤ì„ ë…¸ë“œë¡œ ìƒì„± (ìµœì†Œ 2ë²ˆ ì´ìƒ ë“±ì¥)
        frequent_keywords = [(kw, count) for kw, count in keyword_count.items() if count >= 2]
        frequent_keywords.sort(key=lambda x: x[1], reverse=True)
        
        # ìµœëŒ€ 20ê°œ í‚¤ì›Œë“œë¡œ ì œí•œ
        frequent_keywords = frequent_keywords[:20]
        
        if not frequent_keywords:
            # ë¹ˆë„ê°€ ë‚®ìœ¼ë©´ ëª¨ë“  í‚¤ì›Œë“œ ì‚¬ìš©
            frequent_keywords = [(kw, count) for kw, count in keyword_count.items()][:20]
        
        # í‚¤ì›Œë“œ ë…¸ë“œ ìƒì„±
        nodes = []
        for kw, count in frequent_keywords:
            # ì›ë˜ ëŒ€ì†Œë¬¸ì í˜¼í•© í‚¤ì›Œë“œ ì°¾ê¸°
            original_keyword = kw
            for page in keyword_pages:
                for original_kw in page.keywords_list:
                    if original_kw.lower() == kw:
                        original_keyword = original_kw
                        break
                if original_keyword != kw:
                    break
            
            # í‚¤ì›Œë“œê°€ í¬í•¨ëœ í˜ì´ì§€ë“¤ì˜ URL (ì²« ë²ˆì§¸ í˜ì´ì§€)
            related_pages = keyword_pages_map[kw]
            sample_page = related_pages[0] if related_pages else None
            
            # ë…¸ë“œ í¬ê¸°ëŠ” í‚¤ì›Œë“œ ë¹ˆë„ì— ë¹„ë¡€
            size = min(20 + (count * 5), 50)
            
            # ì¤‘ì‹¬ í‚¤ì›Œë“œì¸ì§€ í™•ì¸
            is_center = kw == keyword.lower()
            if is_center:
                size += 10  # ì¤‘ì‹¬ í‚¤ì›Œë“œ í¬ê¸° ì¦ê°€
            
            # í•´ë‹¹ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” í˜ì´ì§€ë“¤ì˜ ì •ë³´ë¥¼ summaryì— JSONìœ¼ë¡œ ì €ì¥
            page_info_list = []
            for page in related_pages:
                page_info_list.append({
                    'page_id': page.page_id,
                    'title': page.title,
                    'summary': page.summary or "",
                    'url': page.url or "",
                    'keywords': page.keywords_list
                })
            
            import json
            pages_json = json.dumps(page_info_list, ensure_ascii=False)
            
            node = MindmapNode(
                id=f"keyword_{kw}",  # í‚¤ì›Œë“œ ID
                title=original_keyword,  # í‚¤ì›Œë“œë¥¼ ì œëª©ìœ¼ë¡œ ì‚¬ìš©
                keywords=[original_keyword],
                url=sample_page.url if sample_page else "",
                summary=pages_json,  # í˜ì´ì§€ ì •ë³´ë¥¼ JSONìœ¼ë¡œ ì €ì¥
                size=size
            )
            nodes.append(node)
        
        # í‚¤ì›Œë“œ ê°„ ë§í¬ ìƒì„± (ê³µí†µ í˜ì´ì§€ë¥¼ ê°€ì§„ í‚¤ì›Œë“œë“¤)
        links = []
        keyword_list = [kw for kw, _ in frequent_keywords]
        
        for i, kw1 in enumerate(keyword_list):
            for j, kw2 in enumerate(keyword_list):
                if i >= j:
                    continue
                
                # ë‘ í‚¤ì›Œë“œë¥¼ ê³µí†µìœ¼ë¡œ ê°€ì§„ í˜ì´ì§€ ì°¾ê¸°
                pages1 = set(p.page_id for p in keyword_pages_map[kw1])
                pages2 = set(p.page_id for p in keyword_pages_map[kw2])
                common_pages = pages1 & pages2
                
                if len(common_pages) > 0:
                    # ê³µí†µ í˜ì´ì§€ ìˆ˜ì— ê¸°ë°˜í•œ ê°€ì¤‘ì¹˜
                    weight = len(common_pages) / max(len(pages1), len(pages2))
                    
                    if weight >= threshold:
                        link = MindmapLink(
                            source=f"keyword_{kw1}",
                            target=f"keyword_{kw2}",
                            weight=weight,
                            common_keywords=list(common_pages)
                        )
                        links.append(link)
        
        # ì¤‘ì‹¬ ë…¸ë“œ ì„¤ì •
        center_keyword = keyword.lower()
        center_node_id = f"keyword_{center_keyword}"
        
        logger.info(f"í‚¤ì›Œë“œ '{keyword}' ë§ˆì¸ë“œë§µ ìƒì„± ì™„ë£Œ: {len(nodes)}ê°œ í‚¤ì›Œë“œ ë…¸ë“œ, {len(links)}ê°œ ë§í¬")
        
        return MindmapData(
            nodes=nodes,
            links=links,
            center_node=center_node_id
        )
    
    def generate_all_keywords_mindmap(self, threshold: float = None, limit: int = 200) -> MindmapData:
        """ì „ì²´ í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ë§ˆì¸ë“œë§µ ìƒì„±"""
        threshold = threshold or self.threshold
        
        # ëª¨ë“  í˜ì´ì§€ ì¡°íšŒ
        all_pages = db_manager.get_all_pages(limit=limit)
        
        if not all_pages:
            return MindmapData(nodes=[], links=[], center_node="")
        
        # ëª¨ë“  í‚¤ì›Œë“œ ìˆ˜ì§‘ ë° ë¹ˆë„ ê³„ì‚°
        keyword_count = {}
        keyword_pages_map = {}
        keyword_cooccurrence = {}  # í‚¤ì›Œë“œ ë™ì‹œ ì¶œí˜„ ë¹ˆë„
        
        for page in all_pages:
            page_keywords = [kw.lower() for kw in page.keywords_list]
            
            # í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
            for kw in page_keywords:
                keyword_count[kw] = keyword_count.get(kw, 0) + 1
                if kw not in keyword_pages_map:
                    keyword_pages_map[kw] = []
                keyword_pages_map[kw].append(page)
            
            # í‚¤ì›Œë“œ ê°„ ë™ì‹œ ì¶œí˜„ ê´€ê³„ ê³„ì‚°
            for i, kw1 in enumerate(page_keywords):
                for kw2 in page_keywords[i+1:]:
                    if kw1 != kw2:
                        pair = tuple(sorted([kw1, kw2]))
                        keyword_cooccurrence[pair] = keyword_cooccurrence.get(pair, 0) + 1
        
        # ë¹ˆë„ê°€ ë†’ì€ í‚¤ì›Œë“œë“¤ë§Œ ì„ íƒ (ìµœì†Œ 2ë²ˆ ì´ìƒ ë“±ì¥)
        frequent_keywords = [(kw, count) for kw, count in keyword_count.items() if count >= 2]
        frequent_keywords.sort(key=lambda x: x[1], reverse=True)
        
        # ìƒìœ„ 50ê°œ í‚¤ì›Œë“œë§Œ ì„ íƒ
        top_keywords = frequent_keywords[:50]
        selected_keywords = {kw for kw, _ in top_keywords}
        
        # ë…¸ë“œ ìƒì„± (í‚¤ì›Œë“œë³„)
        nodes = []
        max_count = max([count for _, count in top_keywords]) if top_keywords else 1
        
        for keyword, count in top_keywords:
            # í‚¤ì›Œë“œ ë¹ˆë„ì— ë”°ë¼ ë…¸ë“œ í¬ê¸° ê²°ì •
            size = int((count / max_count) * 50) + 10
            
            # í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” í˜ì´ì§€ë“¤ì˜ ì •ë³´ë¥¼ JSONìœ¼ë¡œ ì €ì¥
            page_info_list = []
            space_keys = set()  # ì´ í‚¤ì›Œë“œê°€ ë‚˜íƒ€ë‚˜ëŠ” Spaceë“¤
            for page in keyword_pages_map[keyword]:
                if hasattr(page, 'space_key') and page.space_key:
                    space_keys.add(page.space_key)
                page_info_list.append({
                    'page_id': page.page_id,
                    'title': page.title,
                    'summary': page.summary or "",
                    'url': page.url or "",
                    'keywords': page.keywords_list,
                    'modified_date': page.modified_date,
                    'created_date': page.created_date,
                    'space_key': getattr(page, 'space_key', None)
                })
            
            # ê°€ì¥ ë§ì´ ë‚˜íƒ€ë‚˜ëŠ” Spaceë¥¼ primary spaceë¡œ ì„ íƒ
            space_count = {}
            for page in keyword_pages_map[keyword]:
                if hasattr(page, 'space_key') and page.space_key:
                    space_count[page.space_key] = space_count.get(page.space_key, 0) + 1
            
            primary_space = max(space_count.keys(), key=space_count.get) if space_count else None
            
            import json
            pages_json = json.dumps(page_info_list, ensure_ascii=False)
            
            node = MindmapNode(
                id=f"keyword_{keyword}",
                title=keyword,
                keywords=[keyword],
                url="",  # í‚¤ì›Œë“œ ë…¸ë“œëŠ” URL ì—†ìŒ
                summary=pages_json,  # JSON í˜•íƒœë¡œ í˜ì´ì§€ ì •ë³´ ì €ì¥
                space_key=primary_space,  # ê°€ì¥ ë§ì´ ë‚˜íƒ€ë‚˜ëŠ” Space
                size=size
            )
            nodes.append(node)
        
        # ë§í¬ ìƒì„± (í‚¤ì›Œë“œ ê°„ ë™ì‹œ ì¶œí˜„ ê´€ê³„)
        links = []
        for (kw1, kw2), cooccur_count in keyword_cooccurrence.items():
            if (kw1 in selected_keywords and kw2 in selected_keywords and 
                cooccur_count >= 2):  # ìµœì†Œ 2ë²ˆ ì´ìƒ í•¨ê»˜ ë“±ì¥
                
                # ë™ì‹œ ì¶œí˜„ ë¹ˆë„ì— ë”°ë¼ ë§í¬ ê°€ì¤‘ì¹˜ ê²°ì •
                weight = min(cooccur_count / 10.0, 1.0)  # 0.1 ~ 1.0 ë²”ìœ„
                
                link = MindmapLink(
                    source=f"keyword_{kw1}",
                    target=f"keyword_{kw2}",
                    weight=weight,
                    common_keywords=[kw1, kw2]
                )
                links.append(link)
        
        # ê°€ì¥ ë¹ˆë„ê°€ ë†’ì€ í‚¤ì›Œë“œë¥¼ ì¤‘ì‹¬ ë…¸ë“œë¡œ ì„ íƒ
        center_node = f"keyword_{top_keywords[0][0]}" if top_keywords else ""
        
        logger.info(f"ì „ì²´ í‚¤ì›Œë“œ ë§ˆì¸ë“œë§µ ìƒì„± ì™„ë£Œ: ë…¸ë“œ {len(nodes)}ê°œ, ë§í¬ {len(links)}ê°œ")
        
        return MindmapData(
            nodes=nodes,
            links=links,
            center_node=center_node
        )
    
    def generate_space_mindmap(self, space_key: str, threshold: float = None, limit: int = 100) -> MindmapData:
        """Spaceë³„ ë§ˆì¸ë“œë§µ ìƒì„±"""
        threshold = threshold or self.threshold
        
        logger.info(f"Space ë§ˆì¸ë“œë§µ ìƒì„± ì‹œì‘: space_key={space_key}, threshold={threshold}")
        
        # Spaceì˜ ëª¨ë“  í˜ì´ì§€ ì¡°íšŒ
        space_data = db_manager.get_pages_by_space(space_key, page=1, per_page=limit)
        pages = space_data.get('pages', [])
        
        if not pages:
            logger.warning(f"Spaceì— í˜ì´ì§€ê°€ ì—†ìŒ: {space_key}")
            return MindmapData(nodes=[], links=[], center_node="")
        
        logger.info(f"Space í˜ì´ì§€ ì¡°íšŒ ì™„ë£Œ: {len(pages)}ê°œ")
        
        # í˜ì´ì§€ë“¤ì„ Page ê°ì²´ë¡œ ë³€í™˜ (í•„ìš”í•œ ì†ì„±ë§Œ)
        page_objects = []
        for page_data in pages:
            # page_dataëŠ” dict í˜•íƒœì´ë¯€ë¡œ dict í‚¤ë¡œ ì ‘ê·¼
            page_obj = type('Page', (), {
                'page_id': page_data.get('page_id'),
                'title': page_data.get('title'),
                'keywords_list': page_data.get('keywords', []),
                'url': page_data.get('url', ''),
                'summary': page_data.get('summary', ''),
                'space_key': page_data.get('space_key')
            })()
            page_objects.append(page_obj)
        
        # ê¸°ì¡´ ë§ˆì¸ë“œë§µ ìƒì„± ë¡œì§ ì¬ì‚¬ìš©
        nodes = []
        node_map = {}
        
        # ë…¸ë“œ ìƒì„±
        for page in page_objects:
            keywords_list = page.keywords_list if page.keywords_list else []
            
            node = MindmapNode(
                id=page.page_id,
                title=page.title,
                keywords=keywords_list,
                url=page.url,
                summary=page.summary or "",
                space_key=page.space_key,
                size=max(10, min(30, len(keywords_list) * 2))
            )
            nodes.append(node)
            node_map[page.page_id] = node
        
        # ë§í¬ ìƒì„± (í‚¤ì›Œë“œ ìœ ì‚¬ë„ ê¸°ë°˜)
        links = []
        for i, page1 in enumerate(page_objects):
            for page2 in page_objects[i+1:]:
                similarity = calculate_keyword_similarity(
                    page1.keywords_list, 
                    page2.keywords_list
                )
                
                if similarity >= threshold:
                    common_kws = get_common_keywords(
                        page1.keywords_list, 
                        page2.keywords_list
                    )
                    
                    link = MindmapLink(
                        source=page1.page_id,
                        target=page2.page_id,
                        weight=similarity,
                        common_keywords=common_kws
                    )
                    links.append(link)
        
        # ì¤‘ì‹¬ ë…¸ë“œ ì„ íƒ (ê°€ì¥ ë§ì€ í‚¤ì›Œë“œë¥¼ ê°€ì§„ í˜ì´ì§€)
        center_node = ""
        if nodes:
            center_node = max(nodes, key=lambda n: len(n.keywords)).id
        
        logger.info(f"Space ë§ˆì¸ë“œë§µ ìƒì„± ì™„ë£Œ: space_key={space_key}, ë…¸ë“œ {len(nodes)}ê°œ, ë§í¬ {len(links)}ê°œ")
        
        return MindmapData(
            nodes=nodes,
            links=links,
            center_node=center_node
        )
    
    def generate_combined_mindmap(self, threshold: float = None, limit: int = 200) -> MindmapData:
        """íƒ€ì´í‹€ê³¼ í‚¤ì›Œë“œë¥¼ ëª¨ë‘ ë³´ì—¬ì£¼ëŠ” ê²°í•© ë§ˆì¸ë“œë§µ ìƒì„±"""
        threshold = threshold or self.threshold
        
        logger.info(f"ê²°í•© ë§ˆì¸ë“œë§µ ìƒì„± ì‹œì‘: threshold={threshold}, limit={limit}")
        
        # ëª¨ë“  í˜ì´ì§€ ì¡°íšŒ
        all_pages = db_manager.get_all_pages(limit=limit)
        
        if not all_pages:
            return MindmapData(nodes=[], links=[], center_node="")
        
        # í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
        keyword_count = {}
        keyword_pages_map = {}
        
        for page in all_pages:
            for kw in page.keywords_list:
                kw_lower = kw.lower()
                keyword_count[kw_lower] = keyword_count.get(kw_lower, 0) + 1
                if kw_lower not in keyword_pages_map:
                    keyword_pages_map[kw_lower] = []
                keyword_pages_map[kw_lower].append(page)
        
        # ìì£¼ ë“±ì¥í•˜ëŠ” í‚¤ì›Œë“œë“¤ë§Œ ì„ íƒ (ìµœì†Œ 2ë²ˆ ì´ìƒ)
        frequent_keywords = [(kw, count) for kw, count in keyword_count.items() if count >= 2]
        frequent_keywords.sort(key=lambda x: x[1], reverse=True)
        frequent_keywords = frequent_keywords[:30]  # ìƒìœ„ 30ê°œ í‚¤ì›Œë“œ
        selected_keywords = {kw for kw, _ in frequent_keywords}
        
        nodes = []
        
        # 1. í˜ì´ì§€ ë…¸ë“œ ìƒì„±
        for page in all_pages:
            node = MindmapNode(
                id=f"page_{page.page_id}",
                title=f"ğŸ“„ {page.title}",
                keywords=page.keywords_list,
                url=page.url or "",
                summary=page.summary or "",
                space_key=getattr(page, 'space_key', None),
                size=max(15, min(35, len(page.keywords_list) * 3))  # í˜ì´ì§€ í¬ê¸°
            )
            nodes.append(node)
        
        # 2. í‚¤ì›Œë“œ ë…¸ë“œ ìƒì„±
        max_count = max([count for _, count in frequent_keywords]) if frequent_keywords else 1
        for keyword, count in frequent_keywords:
            # í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” í˜ì´ì§€ ì •ë³´
            page_info_list = []
            for page in keyword_pages_map[keyword]:
                page_info_list.append({
                    'page_id': page.page_id,
                    'title': page.title,
                    'summary': page.summary or "",
                    'url': page.url or ""
                })
            
            import json
            pages_json = json.dumps(page_info_list, ensure_ascii=False)
            
            # í‚¤ì›Œë“œ ë¹ˆë„ì— ë”°ë¥¸ í¬ê¸° ê²°ì •
            size = int((count / max_count) * 25) + 8
            
            node = MindmapNode(
                id=f"keyword_{keyword}",
                title=f"ğŸ·ï¸ {keyword}",
                keywords=[keyword],
                url="",
                summary=pages_json,
                size=size
            )
            nodes.append(node)
        
        # ë§í¬ ìƒì„±
        links = []
        
        # 1. í˜ì´ì§€-í‚¤ì›Œë“œ ë§í¬ (í˜ì´ì§€ê°€ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ê²½ìš°)
        for page in all_pages:
            for kw in page.keywords_list:
                kw_lower = kw.lower()
                if kw_lower in selected_keywords:
                    link = MindmapLink(
                        source=f"page_{page.page_id}",
                        target=f"keyword_{kw_lower}",
                        weight=0.8,  # í˜ì´ì§€-í‚¤ì›Œë“œ ë§í¬ëŠ” ê°•í•œ ì—°ê²°
                        common_keywords=[kw]
                    )
                    links.append(link)
        
        # 2. í˜ì´ì§€-í˜ì´ì§€ ë§í¬ (í‚¤ì›Œë“œ ìœ ì‚¬ë„ ê¸°ë°˜)
        for i, page1 in enumerate(all_pages):
            for page2 in all_pages[i+1:]:
                similarity = calculate_keyword_similarity(
                    page1.keywords_list, 
                    page2.keywords_list
                )
                
                if similarity >= threshold:
                    common_kws = get_common_keywords(
                        page1.keywords_list, 
                        page2.keywords_list
                    )
                    
                    link = MindmapLink(
                        source=f"page_{page1.page_id}",
                        target=f"page_{page2.page_id}",
                        weight=similarity,
                        common_keywords=common_kws
                    )
                    links.append(link)
        
        # 3. í‚¤ì›Œë“œ-í‚¤ì›Œë“œ ë§í¬ (ë™ì‹œ ì¶œí˜„ ê´€ê³„)
        keyword_cooccurrence = {}
        for page in all_pages:
            page_keywords = [kw.lower() for kw in page.keywords_list if kw.lower() in selected_keywords]
            for i, kw1 in enumerate(page_keywords):
                for kw2 in page_keywords[i+1:]:
                    if kw1 != kw2:
                        pair = tuple(sorted([kw1, kw2]))
                        keyword_cooccurrence[pair] = keyword_cooccurrence.get(pair, 0) + 1
        
        for (kw1, kw2), cooccur_count in keyword_cooccurrence.items():
            if cooccur_count >= 2:  # ìµœì†Œ 2ë²ˆ ì´ìƒ í•¨ê»˜ ë“±ì¥
                weight = min(cooccur_count / 5.0, 0.9)  # 0.4 ~ 0.9 ë²”ìœ„
                
                link = MindmapLink(
                    source=f"keyword_{kw1}",
                    target=f"keyword_{kw2}",
                    weight=weight,
                    common_keywords=[kw1, kw2]
                )
                links.append(link)
        
        # ì¤‘ì‹¬ ë…¸ë“œ ì„ íƒ (ê°€ì¥ ë§ì€ í‚¤ì›Œë“œë¥¼ ê°€ì§„ í˜ì´ì§€)
        center_node = ""
        if all_pages:
            center_page = max(all_pages, key=lambda p: len(p.keywords_list))
            center_node = f"page_{center_page.page_id}"
        
        logger.info(f"ê²°í•© ë§ˆì¸ë“œë§µ ìƒì„± ì™„ë£Œ: ë…¸ë“œ {len(nodes)}ê°œ (í˜ì´ì§€ {len(all_pages)}ê°œ, í‚¤ì›Œë“œ {len(frequent_keywords)}ê°œ), ë§í¬ {len(links)}ê°œ")
        
        return MindmapData(
            nodes=nodes,
            links=links,
            center_node=center_node
        )

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
mindmap_service = MindmapService()